# 利用强化学习训练 Agent 玩 Atari 游戏
## 复现论文
### 预处理
Atari游戏返回的画面大小为210x160x3的彩色图，首先将其转化为灰度图。
图片顶端为计分栏应裁掉以免造成干扰，生成160x160的样本图。
根据论文将图片resize为84x84，同时将四幅连续的图片组成一个样本训练神经网络  
### 神经网络结构
输入：84x84x4的图片  
第一层卷积：16个8x8卷积核，步长为4  
第二层卷积：32个4x4卷积核，步长为2  
第三层全连接：输入32x9x9，输出256  
第四层全连接：输入256，输出6对应6个动作  
（我们考虑在卷积与全连接之间加一层最大池化层以增强特征，但由于训练轮数限制暂不能比较出两者的差异）  
### 记忆池
通过队列或数组存储每一步的[state, action, reward, next_state, done]
### 行为决策：ϵ-greedy 策略
ϵ-greedy 策略兼具探索与利用的功能，在已知与未知之间进行了平衡  
通过引入随机行为，增强了对游戏的探索，同时能产生更多有用的样本   
样本的随机化破坏了该相关性，从而减少了更新的方差   
并且随着训练的进行逐渐减少随机行为，使最终的模型能利用已有的训练记忆更好地进行决策
### 神经网络优化
根据论文的算法，运用贝尔曼公式
Q(s,a) ← Q(s,a)+α[r+γmaxa'Q(s',a')−Q(s,a)]   
损失函数为  
L(θ)=E[(TargetQ−Q(s,a;θ))^2]  
通过对loss的梯度下降调整神经网络参数  
同时为了降低相关性，使用了两个初始相同的神经网络target-netwoek与 eval-network  
eval-network一直训练更新，而target-network则周期性地与eval-network同步  








